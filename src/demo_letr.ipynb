{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LETR Basic Usage Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "import torchvision.transforms.functional as functional\n",
    "import torch.nn.functional as F\n",
    "from glob import glob\n",
    "from models import build_model\n",
    "from util.misc import nested_tensor_from_tensor_list\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        for t in self.transforms:\n",
    "            format_string += \"\\n\"\n",
    "            format_string += \"    {0}\".format(t)\n",
    "        format_string += \"\\n)\"\n",
    "        return format_string\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = functional.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, img):\n",
    "        return functional.to_tensor(img)\n",
    "\n",
    "def resize(image, size, max_size=None):\n",
    "    # size can be min_size (scalar) or (w, h) tuple\n",
    "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
    "        w, h = image_size\n",
    "        if max_size is not None:\n",
    "            min_original_size = float(min((w, h)))\n",
    "            max_original_size = float(max((w, h)))\n",
    "            if max_original_size / min_original_size * size > max_size:\n",
    "                size = int(round(max_size * min_original_size / max_original_size))\n",
    "        if (w <= h and w == size) or (h <= w and h == size):\n",
    "            return (h, w)\n",
    "        if w < h:\n",
    "            ow = size\n",
    "            oh = int(size * h / w)\n",
    "        else:\n",
    "            oh = size\n",
    "            ow = int(size * w / h)\n",
    "        return (oh, ow)\n",
    "\n",
    "    def get_size(image_size, size, max_size=None):\n",
    "        if isinstance(size, (list, tuple)):\n",
    "            return size[::-1]\n",
    "        else:\n",
    "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
    "\n",
    "    size = get_size(image.size, size, max_size)\n",
    "    rescaled_image = functional.resize(image, size)\n",
    "\n",
    "    return rescaled_image\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, sizes, max_size=None):\n",
    "        assert isinstance(sizes, (list, tuple))\n",
    "        self.sizes = sizes\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        size = self.sizes\n",
    "        return resize(img, size, self.max_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_positions(lines, heatmap_scale=(128, 128), im_shape=None): # TODO: check if still works for non square heatmap\n",
    "    if len(lines) == 0:\n",
    "        return []\n",
    "    fy, fx = heatmap_scale[1] / im_shape[0], heatmap_scale[0] / im_shape[1]\n",
    "\n",
    "    lines[:, :, 0] = np.clip(lines[:, :, 0] * fx, 0, heatmap_scale[0] - 1e-4)\n",
    "    lines[:, :, 1] = np.clip(lines[:, :, 1] * fy, 0, heatmap_scale[1] - 1e-4)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Pre-trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/home/kallelis/PrimitiveExtraction/PrimitiveExtraction/Detection/LETR/exp/exp/res101_stage2_focal/checkpoints/'\n",
    "npz_dir = ckpt_path + f'/eida_final_npz/'\n",
    "# os.makedirs(npz_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain checkpoints\n",
    "checkpoint = torch.load(f'{ckpt_path}checkpoint0024.pth', map_location='cpu')\n",
    "\n",
    "# load model\n",
    "args = checkpoint['args']\n",
    "model, _, postprocessors = build_model(args)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = Path(\"/home/kallelis/PrimitiveExtraction/PrimitiveExtraction/data/eida_final/images\")\n",
    "test_size = 1100\n",
    "normalize = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize([0.538, 0.494, 0.453], [0.257, 0.263, 0.273]),\n",
    "        Resize([test_size]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name in os.listdir(full_path): \n",
    "    print(image_name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "not_computed = os.listdir('/home/kallelis/PrimitiveExtraction/PrimitiveExtraction/data/eida_final/images')\n",
    "computed=os.listdir('/home/kallelis/PrimitiveExtraction/PrimitiveExtraction/Detection/LETR/exp/exp/res101_stage2_focal/checkpoints/eida_final_npz')\n",
    "remaining = list(set(not_computed) - set(computed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name in os.listdir(full_path): \n",
    "    # im_name ='ms2_0243_716,1556,778,407'\n",
    "    im_name = image_name[:-4]\n",
    "    if (f\"{im_name}.npz\" in computed):\n",
    "        # print(f\"skipping {im_name}\")\n",
    "        continue\n",
    "    print(f\"computing {im_name}\")\n",
    "    im_path = full_path / f'{im_name}.jpg'\n",
    "\n",
    "    raw_img = Image.open(im_path).convert(\"RGB\")\n",
    "    w,h = raw_img.size\n",
    "    # original = original[:,:,:3]\n",
    "    # raw_img = original\n",
    "    # h, w = raw_img.shape[0], raw_img.shape[1]\n",
    "    orig_size = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "\n",
    "    img = normalize(raw_img)\n",
    "    inputs = nested_tensor_from_tensor_list([img])\n",
    "    # inputs = inputs.to('cuda')\n",
    "    # plt.axis('off')\n",
    "    # plt.imshow(raw_img)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    outputs = outputs[0]\n",
    "    out_logits, out_line = outputs['pred_logits'], outputs['pred_lines']\n",
    "    prob = F.softmax(out_logits, -1)\n",
    "    scores, labels = prob[..., :-1].max(-1)\n",
    "    img_h, img_w = orig_size.unbind(0)\n",
    "    # scale_fct = torch.unsqueeze(torch.stack([img_w, img_h, img_w, img_h], dim=0), dim=0).to('cuda')\n",
    "    scale_fct = torch.unsqueeze(torch.stack([img_w, img_h, img_w, img_h], dim=0), dim=0)\n",
    "\n",
    "    lines = out_line * scale_fct[:, None, :]\n",
    "    # lines = lines.view(1000, 2, 2)\n",
    "    # lines = lines.flip([-1])# this is yxyx format\n",
    "    scores = scores.detach().cpu().numpy()\n",
    "    results = [\n",
    "        {\"scores\": s, \"labels\": l, \"lines\": b}\n",
    "        for s, l, b in zip(scores, labels, lines)\n",
    "    ]\n",
    "\n",
    "    pred_logits = outputs[\"pred_logits\"]\n",
    "    bz = pred_logits.shape[0]\n",
    "    assert bz == 1\n",
    "    query = pred_logits.shape[1]\n",
    "\n",
    "    # rst = results[0][\"lines\"].detach()\n",
    "    rst = results[0][\"lines\"]\n",
    "\n",
    "    pred_lines = rst.view(query, 2, 2)\n",
    "\n",
    "    # pred_lines = pred_lines.flip([-1])  # this is yxyx format\n",
    "\n",
    "    h, w = orig_size.tolist()\n",
    "    # pred_lines = scale_positions(pred_lines.detach().cpu().numpy(), (128, 128), (h, w))\n",
    "    pred_lines = scale_positions(pred_lines.cpu().numpy(), (128, 128), (h, w))\n",
    "\n",
    "    score = results[0][\"scores\"]\n",
    "    line = pred_lines\n",
    "    score_idx = np.argsort(-score)\n",
    "    line = line[score_idx]\n",
    "    score = score[score_idx]\n",
    "\n",
    "    npz_save_path = npz_dir + f'{im_name}.npz'\n",
    "    np.savez(npz_save_path, **{\"lines\": line, \"line_scores\": score})\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('/home/kallelis/PrimitiveExtraction/PrimitiveExtraction/Detection/LETR/exp/exp/res101_stage2_focal/checkpoints/eida_final_npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # out_logits, out_line = outputs['pred_logits'], outputs['pred_lines']\n",
    "# # prob = F.softmax(out_logits, -1)\n",
    "# # scores, labels = prob[..., :-1].max(-1)\n",
    "# # img_h, img_w = orig_size.unbind(0)\n",
    "# # scale_fct = torch.unsqueeze(torch.stack([img_w, img_h, img_w, img_h], dim=0), dim=0).to('cuda')\n",
    "# # lines = out_line * scale_fct[:, None, :]\n",
    "\n",
    "\n",
    "# out_logits, out_line = outputs[\"pred_logits\"], outputs[\"pred_lines\"]\n",
    "\n",
    "# # assert orig_size.shape[1] == 2\n",
    "\n",
    "# prob = F.softmax(out_logits, -1)\n",
    "# scores, labels = prob[..., :-1].max(-1)\n",
    "\n",
    "# # convert to [x0, y0, x1, y1] format\n",
    "# img_h, img_w = orig_size.unbind(0)\n",
    "# scale_fct = torch.unsqueeze(torch.stack([img_w, img_h, img_w, img_h], dim=0), dim=0).to('cuda')\n",
    "# lines = out_line * scale_fct[:, None, :]\n",
    "\n",
    "\n",
    "# results = [\n",
    "#     {\"scores\": s, \"labels\": l, \"lines\": b}\n",
    "#     for s, l, b in zip(scores, labels, lines)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]['scores'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"pred_logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_size = torch.stack([orig_size] * 1, dim=0).to('cuda')\n",
    "# results = postprocessors[\"line\"](outputs, orig_size, \"prediction\", unbind_dim=1)\n",
    "pred_logits = outputs[\"pred_logits\"]\n",
    "bz = pred_logits.shape[0]\n",
    "assert bz == 1\n",
    "query = pred_logits.shape[1]\n",
    "\n",
    "rst = results[0][\"lines\"].detach()\n",
    "pred_lines = rst.view(query, 2, 2)\n",
    "\n",
    "# pred_lines = pred_lines.flip([-1])  # this is yxyx format\n",
    "\n",
    "h, w = orig_size.tolist()\n",
    "pred_lines[:, :, 0] = pred_lines[:, :, 0] * (128)\n",
    "pred_lines[:, :, 0] = pred_lines[:, :, 0] / h\n",
    "pred_lines[:, :, 1] = pred_lines[:, :, 1] * (128)\n",
    "pred_lines[:, :, 1] = pred_lines[:, :, 1] / w\n",
    "\n",
    "score = results[0][\"scores\"]\n",
    "line = pred_lines.detach().cpu().numpy()\n",
    "\n",
    "score_idx = np.argsort(-score)\n",
    "line = line[score_idx]\n",
    "score = score[score_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_dir = ckpt_path + f'/eida_final_npz/'\n",
    "os.makedirs(npz_dir, exist_ok=True)\n",
    "npz_save_path = npz_dir + f'{im_name}.npz'\n",
    "np.savez(npz_save_path, **{\"lines\": line, \"line_scores\": score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = score >= 0.3\n",
    "keep = keep.squeeze()\n",
    "final_lines = lines[0][keep]\n",
    "final_lines = final_lines.reshape(final_lines.shape[0], -1)\n",
    "fig = plt.figure()\n",
    "plt.imshow(raw_img)\n",
    "for tp_id, line_to_plot in enumerate(final_lines):\n",
    "    y1, x1, y2, x2 = line_to_plot.detach().cpu().numpy() # this is yxyx\n",
    "    p1 = (x1, y1)\n",
    "    p2 = (x2, y2)\n",
    "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], linewidth=1, color='red', zorder=1)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "#plt.savefig(\"../figures/demo_result.png\", dpi=300, bbox_inches='tight', pad_inches = 0)\n",
    "#plt.close(fig)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('detr': conda)",
   "metadata": {
    "interpreter": {
     "hash": "410c54daf323f5212ce889dbd0c7b13970b5bad95aeecb5b05a0f5b22af8bc3f"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
