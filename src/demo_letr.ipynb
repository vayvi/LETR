{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LETR Basic Usage Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "import torchvision.transforms.functional as functional\n",
    "import torch.nn.functional as F\n",
    "from glob import glob\n",
    "from models import build_model\n",
    "from util.misc import nested_tensor_from_tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        for t in self.transforms:\n",
    "            format_string += \"\\n\"\n",
    "            format_string += \"    {0}\".format(t)\n",
    "        format_string += \"\\n)\"\n",
    "        return format_string\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = functional.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, img):\n",
    "        return functional.to_tensor(img)\n",
    "\n",
    "def resize(image, size, max_size=None):\n",
    "    # size can be min_size (scalar) or (w, h) tuple\n",
    "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
    "        w, h = image_size\n",
    "        if max_size is not None:\n",
    "            min_original_size = float(min((w, h)))\n",
    "            max_original_size = float(max((w, h)))\n",
    "            if max_original_size / min_original_size * size > max_size:\n",
    "                size = int(round(max_size * min_original_size / max_original_size))\n",
    "        if (w <= h and w == size) or (h <= w and h == size):\n",
    "            return (h, w)\n",
    "        if w < h:\n",
    "            ow = size\n",
    "            oh = int(size * h / w)\n",
    "        else:\n",
    "            oh = size\n",
    "            ow = int(size * w / h)\n",
    "        return (oh, ow)\n",
    "\n",
    "    def get_size(image_size, size, max_size=None):\n",
    "        if isinstance(size, (list, tuple)):\n",
    "            return size[::-1]\n",
    "        else:\n",
    "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
    "\n",
    "    size = get_size(image.size, size, max_size)\n",
    "    rescaled_image = functional.resize(image, size)\n",
    "\n",
    "    return rescaled_image\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, sizes, max_size=None):\n",
    "        assert isinstance(sizes, (list, tuple))\n",
    "        self.sizes = sizes\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        size = self.sizes\n",
    "        return resize(img, size, self.max_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Pre-trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain checkpoints\n",
    "checkpoint = torch.load('/home/kallelis/PrimitiveExtraction/PrimitiveExtraction/Detection/LETR/exp/res50_stage2/checkpoints/checkpoint.pth', map_location='cpu')\n",
    "\n",
    "# load model\n",
    "args = checkpoint['args']\n",
    "model, _, postprocessors = build_model(args)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Demo Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = Path(\"/home/kallelis/PrimitiveExtraction/PrimitiveExtraction/Detection/LETR/\")\n",
    "# # folder_path = root / \"data/wireframe_processed/val2017\"\n",
    "\n",
    "# # for im_path in glob(str(folder_path / \"*.png\")):\n",
    "# #     break\n",
    "\n",
    "# # folder_path = root / \"data/wireframe_processed/val2017\"\n",
    "# folder_path = root / \"data/synthetic_processed/val\"\n",
    "\n",
    "# for im_path in glob(str(folder_path / \"*.png\")):\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = \"/home/kallelis/PrimitiveExtraction/PrimitiveExtraction/data/full_sample_resized/diagram18.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "original = plt.imread(im_path)\n",
    "original = original[:,:,:3]\n",
    "# edge_im = plt.imread(my_test_edge)[:,:,0]\n",
    "# h,w = original.shape[0], original.shape[1]\n",
    "\n",
    "# edge_im = cv2.resize(edge_im, (w, h))\n",
    "# blurred_image = cv2.blur(original, (9,9))\n",
    "# original = original[:,:,:3]\n",
    "# mask = np.stack([edge_im]*3, axis=2)\n",
    "\n",
    "# print(edge_im.shape)\n",
    "# raw_img = np.where(mask>0, original, blurred_image)\n",
    "raw_img = original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = raw_img.shape[0], raw_img.shape[1]\n",
    "orig_size = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "# normalize image\n",
    "test_size = 1100\n",
    "normalize = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize([0.538, 0.494, 0.453], [0.257, 0.263, 0.273]),\n",
    "        Resize([test_size]),\n",
    "    ])\n",
    "img = normalize(raw_img)\n",
    "inputs = nested_tensor_from_tensor_list([img])\n",
    "plt.axis('off')\n",
    "plt.imshow(raw_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = outputs[0]\n",
    "out_logits, out_line = outputs['pred_logits'], outputs['pred_lines']\n",
    "prob = F.softmax(out_logits, -1)\n",
    "scores, labels = prob[..., :-1].max(-1)\n",
    "img_h, img_w = orig_size.unbind(0)\n",
    "scale_fct = torch.unsqueeze(torch.stack([img_w, img_h, img_w, img_h], dim=0), dim=0)\n",
    "lines = out_line * scale_fct[:, None, :]\n",
    "lines = lines.view(1000, 2, 2)\n",
    "lines = lines.flip([-1])# this is yxyx format\n",
    "scores = scores.detach().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = scores >= 0.3\n",
    "keep = keep.squeeze()\n",
    "final_lines = lines[keep]\n",
    "final_lines = final_lines.reshape(final_lines.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(raw_img)\n",
    "for tp_id, line in enumerate(final_lines):\n",
    "    y1, x1, y2, x2 = line.detach().numpy() # this is yxyx\n",
    "    p1 = (x1, y1)\n",
    "    p2 = (x2, y2)\n",
    "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], linewidth=1, color='red', zorder=1)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "#plt.savefig(\"../figures/demo_result.png\", dpi=300, bbox_inches='tight', pad_inches = 0)\n",
    "#plt.close(fig)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('detr': conda)",
   "metadata": {
    "interpreter": {
     "hash": "410c54daf323f5212ce889dbd0c7b13970b5bad95aeecb5b05a0f5b22af8bc3f"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
